---
title: "Project2_Final"
author: "Semyon Toybis"
date: "2024-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readxl)
library(caret)
library(tidyverse)
library(VIM)
```

## Import Excel tables:

Below we import the excel data from Github

```{r}
stu_data_url <- 'https://github.com/amedina613/Data624-Project2/raw/refs/heads/main/StudentData.xlsx'

dest_file_stu_data <- 'StudentData.xlsx'

stu_eval_url <- 'https://github.com/amedina613/Data624-Project2/raw/refs/heads/main/StudentEvaluation.xlsx'
  
dest_file_stu_eval <- 'StudentEvaluation.xlsx'

download.file(stu_data_url, dest_file_stu_data, mode = 'wb')
download.file(stu_eval_url, dest_file_stu_eval, mode = 'wb')

stu_data_raw <- read_excel('StudentData.xlsx',col_names = T)
stu_eval_raw <- read_excel('StudentEvaluation.xlsx',col_names = T)

stu_data_raw <- as.data.frame(stu_data_raw)
stu_eval_raw <- as.data.frame(stu_eval_raw)
```

## Data exploration

First, we check the structure of the data set we will use to train and test the model.

```{r}
str(stu_data_raw)
```

Next, we check the amount of missing values in each column.

```{r}
missing_values <- colSums(is.na(stu_data_raw))

print(missing_values)
```

We also visualize missing data:

```{r}
library(naniar)
gg_miss_var(stu_data_raw)
```

```{r}
vis_miss(stu_data_raw)
```

## Data preparation

### Dummy variables

The Brand Code column is a categorical variable. We will have to convert it to dummy variables to perform knn imputation.

First, we convert the column to a factor.

```{r}
stu_data_raw$`Brand Code` <- as.factor(stu_data_raw$`Brand Code`)
```

Next, we convert the Brand Code to dummy variables:

```{r}
dummy_variables <- dummyVars(~., data = stu_data_raw, fullRank = T)
training_data <- predict(dummy_variables, newdata = stu_data_raw)
```

```{r}
head(training_data)
```

We now have three dummy variable columns: Brand Code B, Brand Code C, and Brand Code D with zeros and ones. If all three columns have zero, then it means the Brand is A.

### Center, Scale, KNN

Next, we center and scale the data and perform KNN imputation for missing values.

```{r}
preProcess_model <- preProcess(training_data, method = c("center", "scale", "knnImpute"))
training_data_imputed <- predict(preProcess_model, training_data)
```

We now have zero missing values:

```{r}
colSums(is.na(training_data_imputed))
```

### Near Zero Variance predictors

Near zero variance predictors are identified and removed from the imputed numeric data set. These predictors have little variability and do not contribute meaningfully to analysis or modeling.

There is only one variable with near zero variance:

```{r}
colnames(training_data_imputed)[nearZeroVar(training_data_imputed)]
```

```{r}
training_data_imputed <- training_data_imputed[, -nearZeroVar(training_data_imputed)]

```

Our working training data-set is training_data_imputed

```{r}
head(training_data_imputed)
```

Before further analysis continues, the final data set should be

```{r}
training_data_imputed <- as.data.frame(training_data_imputed)
```

### Train-Test Split

The data is split into 80% training and 20% testing to train and validate models.

```{r}
set.seed(321)  

index <- createDataPartition(training_data_imputed$PH, p = 0.8, list = FALSE)

train_x <- training_data_imputed[index, -which(names(training_data_imputed) == "PH")]
train_y <- training_data_imputed[index, "PH"]
test_x <- training_data_imputed[-index, -which(names(training_data_imputed) == "PH")]
test_y <- training_data_imputed[-index, "PH"]


```

## Correlation matrix: PH vs predictors

We evaluate the relationships between pH (our target) and all available predictors to gain a preliminary understanding of which factors might drive our outcome. Using correlation and visualization we identify the variables are most strongly associated with pH.

The table of correlations ranks predictors based on their correlation with the target variable, pH. Mnf.Flow shows the strongest relationship with pH (negative correlation of -0.445), Bowl.Setpoint (positive correlation of 0.346) is has the strongest positive correlation and may indicate a role in influencing pH -- although these correlations are moderate at best.

```{r, message=FALSE}
library(GGally)
# Compute correlations between 'PH' and all other predictors
correlation_values <- training_data_imputed %>%
    summarise(across(.cols = everything(), 
                     .fns = ~ cor(., training_data_imputed$PH, use = "complete.obs"), 
                     .names = "cor_{col}")) %>%
    pivot_longer(cols = everything(), names_to = "Predictor", values_to = "Correlation") %>%
    mutate(Predictor = gsub("cor_", "", Predictor)) %>%
    filter(Predictor != "PH") %>% 
    arrange(desc(abs(Correlation)))

# Output sorted list of correlations
print(correlation_values)

# Create scatterplot matrix for PH against other predictors
correlation_matrix <- ggpairs(
  data = training_data_imputed,
  columns = which(names(training_data_imputed) == "PH"):ncol(training_data_imputed),
  upper = list(continuous = wrap("cor", size = 3)),
  title = "Correlation Scatterplot Matrix for PH") +
  theme(
    axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 7),
    strip.text = element_text(size = 7),
    plot.title = element_text(size = 12))

# Display the scatterplot matrix
print(correlation_matrix)
```

## Baseline model

We begin our modeling exploration with a simple linear regression, focused on the top predictors identified in the correlation analysis. We fit and visualize straightforward models to see how well a single predictor explains pH variance. The baseline helps us understand whether simple relationships exist and serves as a reference point for improvement. Although we do not expect this simple model to be the best, it provides a clear benchmark for gauging progress.

Bowl.Setpoint has slight upward trend in pH as the setpoint increases, with an adjusted R-squared of 0.119. This suggests that Bowl.Setpoint explains approximately 11.9% of the variance in pH. Mnf.Flow has a negative slope association, showing that pH decreases as Mnf.Flow increases, with a higher adjusted R-squared of 0.198.

Mnf.Flow is a stronger predictor of pH than Bowl.Setpoint but both are relatives weak predictors for pH. Their low R-squared values point to using additional predictors or more complex models to capture the remaining variance.

```{r}
# Scatterplot with regression line for PH vs. Bowl.Setpoint

```

## Model Training

### Support Vector Machine (SVM)

Training a non-linear SVM model to predict PH values.

```{r}
control <- trainControl(method = "cv", number = 5)

set.seed(123)

svm_model <- train(
  x = train_x, 
  y = train_y, 
  method = "svmRadial", 
  tuneLength = 10,  
  trControl = control  
)
```

```{r}
plot(svm_model)
```

```{r}
plot(varImp(svm_model))
```

Predicting PH values using the trained SVM model on the test set.

```{r}
svm_predictions <- predict(svm_model, newdata = test_x)
```

Evaluate the model.

```{r}
svm_rmse <- RMSE(svm_predictions, test_y)
svm_r2 <- R2(svm_predictions, test_y)
```

```{r}
cat("SVM RMSE:", svm_rmse, "\n")
cat("SVM R²:", svm_r2, "\n")
```

This RMSE score is high at 0.71, and the R² is 0.51 indicating there is room for improvement in the model

### Ridge Regression

Training a Ridge Regression model with cross-validation to find the optimal λ (lambda) value and predict pH values.

```{r}

set.seed(345)

ridge_model <- train(
  x = train_x, 
  y = train_y, 
  method = "glmnet", 
  tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 20)), 
  metric = "Rsquared",  
  trControl = control,    
  preProc = c("center", "scale")  
)

```

```{r}
plot(ridge_model)
```

```{r}
plot(varImp(ridge_model))
```

Using the trained Ridge model to predict pH values for the test set.

```{r}
#  Predict and evaluate the Ridge Regression model
ridge_predictions <- predict(ridge_model, newdata = test_x)
```

Evaluate the model

```{r}
ridge_rmse <- RMSE(ridge_predictions, test_y)
ridge_r2 <- caret::R2(ridge_predictions, test_y)
```

```{r}
cat("Ridge Regression RMSE:", ridge_rmse, "\n")
cat("Ridge Regression R²:", ridge_r2, "\n")
```

The low R² value indicates there's still a large amount of unexplained variance.

### Lasso

We model a Lasso regression to leverage its feature selection, model complexity reduction attributes. Lasso applies a penalty to shrink less important coefficients toward zero resulting in smaller set non-zero important features . By focusing on the most critical variables, we may be able to generate a parsimonious model that may outperform our linear regression baselines.

The Lasso regression results reveal a tuned model with optimal parameters: alpha = 0.1 and lambda = 0.0012, The variable importance rankings highlight Oxygen.Filler as the most influential predictor, followed by Carb.Rel, PC.Volume, and Density. These top features identify key predictors of pH.

The Lasso regression achieves effective dimensionality reduction but te relatively modest R-squared value suggests that further exploration may be necessary. The R-squared of 0.354 is lower than the adjusted R-squared of 0.4072 from the multiple linear regression (MLR) model and may not be a suitable model for this dataset.

```{r}
library(glmnet)

set.seed(321)

#Define cross-validation method
cross_val_10 <- trainControl(method = "cv", number = 10)

lasso_model <- train(
  x = train_x,
  y = train_y,
  method = "lasso",
  trControl = cross_val_10,
  tuneLength = 20)

```

```{r}
plot(lasso_model)
```

```{r}
plot(varImp(lasso_model))
```

Next, we predict the test set with the Lasso model:

```{r}
lasso_predictions <- predict(lasso_model, newdata = test_x)
```

```{r}
postResample(lasso_predictions, test_y)
```

### Neural Network

To capture non-linear and potentially complex relationships, we train a neural network model.

The neural network model achieves an R-squared of 0.477, surpassing the performance of the Lasso regression (R-squared = 0.354) and coming close to the multiple linear regression (MLR) model’s R-squared of 0.407. The optimal parameters are size = 9 (number of hidden units) and decay = 0.5 (regularization parameter). Overall, the neural network model has a moderate results.

The tuning results graph shows that the combination of decay and hidden units significantly impacts model performance, with RMSE values improving as the decay and size are fine-tuned. The variable importance plot highlights Carb.Flow, Hyd.Pressure1, and Mnf.Flow as key predictors.

The residual plot suggests minimal systematic errors, with residuals centered around zero across predicted values.

```{r}
library(nnet)

# Define a tuning grid
nnetGrid <- expand.grid(
  size = 1:10,
  decay = c(0.001, 0.01, 0.1, 0.5))

```

```{r}
library(foreach)
library(doParallel)
```

We create a cluster that is two less than the number of system cores.

```{r}
cluster0 <- makeCluster(detectCores()-2)
```

Next, we start the cluster, train the model, and close the cluster.

```{r}

registerDoParallel(cluster0)

# Train neural network
set.seed(321)
nnetTuned <- train(
  x = train_x,
  y = train_y,
  method = "nnet",
  tuneGrid = nnetGrid,
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  linout = TRUE,
  trace = FALSE,
  maxit = 500,
  MaxNWts = 10 * (ncol(train_x) + 1) + 10 + 1)

stopCluster(cluster0)
```

```{r}
plot(nnetTuned)
```

```{r}
plot(varImp(nnetTuned))
```

Next, we try predicting the test set with the neural network:

```{r}
nnet_predictions <- predict(nnetTuned, newdata = test_x)
```

```{r}
postResample(nnet_predictions, test_y)
```

### Random Forest model

Below we try fitting a random forest model to the data.

One of the tuning parameters for random forests is the number of predictors, k, to choose (referred to as $m_{try}$ - it is recommended to set this parameter to one-third of the number of predictors. Number of trees is an additional parameter - it is recommended to set this to at least 1000.

Below, we create a tune grid for $m_{try}$ parameter.

```{r}
mtry <- seq(from = 1, to = (ncol(train_x)), by = 4)
mtry <- as.data.frame(mtry)
```

We use parallel computing to reduce the run time for training the model.

We create a cluster that is two less than the number of system cores.

```{r}
cluster <- makeCluster(detectCores()-2)
```

Next, we start the cluster, train the model, and close the cluster.

```{r, warning=FALSE}
registerDoParallel(cluster)

set.seed(50)

rf_model <- train(x = train_x, y = train_y, method = 'rf',
                  tuneGrid = mtry,
                  trControl = trainControl(method = 'cv', number = 10, allowParallel = T))

stopCluster(cluster)
```

```{r}
rf_model
```

Below is a a plot of the cross-validation RMSE

```{r}
plot(rf_model)
```

Below is a plot of the variable importance:

```{r}
plot(varImp(rf_model))
```

Next, we try predicting the test set with the random forest model

```{r}
rf_predictions <- predict(rf_model, newdata = test_x)
```

We compare the predictions with the actual values:

```{r}
postResample(pred = rf_predictions, obs = test_y)
```

### Gradient boosted model

Next, we try fitting a gradient boosted regression tree model to the data. The parameters for this model include tree depth, number of trees, and shrinkage parameter which controls how much of a predicted value from a previous iteration is added to the current iteration (values of \<0.01 are recommended). Lastly, the bagging fraction is kept constant at 0.5.

Below, we create the tuning grid:

```{r}
gbmGrid <- expand.grid(interaction.depth = seq(1,7, by = 2),
                       shrinkage = c(0.01,0.1),
                       n.trees = seq(100,1000, by = 50),
                       n.minobsinnode = seq(5,30, by = 5))
```

We will use parallel computing to reduce the training time. Below we start the cluster, train the model, and close the cluster.

```{r}
cluster2 <- makeCluster(detectCores()-2)
```

```{r}
registerDoParallel(cluster2)

set.seed(100)

gbm_model <- train(x = train_x, y = train_y, , method = 'gbm',
                  tuneGrid = gbmGrid,
                  trControl = trainControl(method = 'cv', number = 10, allowParallel = T),
                  verbose = F)

stopCluster(cluster2)
```

```{r}
gbm_model
```

```{r}
plot(gbm_model)
```

Below are the parameters for the final model:

```{r}
gbm_model$bestTune
```

Next, we try predicting the test set with the gradient boosted model

```{r}
gbm_predictions <- predict(gbm_model, newdata = test_x)
```

We compare the predictions with the actual values:

```{r}
postResample(pred = gbm_predictions, obs = test_y)
```

### Model Performance Metrics

Below are the performance metrics on the test set for the various models

```{r}
svm_metrics <- as.data.frame(postResample(pred = svm_predictions, obs = test_y))
ridge_Metrics <- as.data.frame(postResample(pred = ridge_predictions, obs = test_y))
lasso_Metrics <- as.data.frame(postResample(pred = lasso_predictions, obs = test_y))
nnet_Metrics <- as.data.frame(postResample(pred = nnet_predictions, obs = test_y))
rf_Metrics <- as.data.frame(postResample(pred = rf_predictions, obs = test_y))
gbm_metrics <- as.data.frame(postResample(pred = gbm_predictions, obs = test_y))

```

```{r}
model_performance <- cbind(svm_metrics, ridge_Metrics, lasso_Metrics, nnet_Metrics,
                           rf_Metrics, gbm_metrics)
colnames(model_performance) <- c('SVM', 'Ridge', 'Lasso', 'NNet', 'RF', 'GBM')
model_performance <- as.data.frame(t(model_performance))
```

```{r}
model_performance
```
